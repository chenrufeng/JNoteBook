{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "model = LinearRegressionModel(input_dim, output_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
      "[ 0.  2.  4.  6.  8. 10. 12. 14. 16. 18. 20.]\n",
      "[ 0.  2.  4.  6.  8. 10. 12. 14. 16. 18. 20.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_values = [i for i in range(11)]\n",
    "print(x_values)\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "print(x_train)\n",
    "x_train.reshape(-1,1)\n",
    "print(x_train)\n",
    "\n",
    "y_values = [i*2 for i in range(11)]\n",
    "print(y_values)\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "print(y_train)\n",
    "y_train.reshape(-1,1)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch +=1\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    labels = torch.from_numpy(y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # 反向传播，计算个权重\n",
    "    loss.backward()\n",
    "    # 更新权重参数\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000e-04, requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.linspace(-math.pi, math.pi, 200, device=device, dtype=dtype)\n",
    "torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.tensor(0.0001,device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 nan\n",
      "199 nan\n",
      "299 nan\n",
      "399 nan\n",
      "499 nan\n",
      "599 nan\n",
      "699 nan\n",
      "799 nan\n",
      "899 nan\n",
      "999 nan\n",
      "1099 nan\n",
      "1199 nan\n",
      "1299 nan\n",
      "1399 nan\n",
      "1499 nan\n",
      "1599 nan\n",
      "1699 nan\n",
      "1799 nan\n",
      "1899 nan\n",
      "1999 nan\n",
      "2099 nan\n",
      "2199 nan\n",
      "2299 nan\n",
      "2399 nan\n",
      "2499 nan\n",
      "2599 nan\n",
      "2699 nan\n",
      "2799 nan\n",
      "2899 nan\n",
      "2999 nan\n",
      "3099 nan\n",
      "3199 nan\n",
      "3299 nan\n",
      "3399 nan\n",
      "3499 nan\n",
      "3599 nan\n",
      "3699 nan\n",
      "3799 nan\n",
      "3899 nan\n",
      "3999 nan\n",
      "4099 nan\n",
      "4199 nan\n",
      "4299 nan\n",
      "4399 nan\n",
      "4499 nan\n",
      "4599 nan\n",
      "4699 nan\n",
      "4799 nan\n",
      "4899 nan\n",
      "4999 nan\n",
      "5099 nan\n",
      "5199 nan\n",
      "5299 nan\n",
      "5399 nan\n",
      "5499 nan\n",
      "5599 nan\n",
      "5699 nan\n",
      "5799 nan\n",
      "5899 nan\n",
      "5999 nan\n",
      "6099 nan\n",
      "6199 nan\n",
      "6299 nan\n",
      "6399 nan\n",
      "6499 nan\n",
      "6599 nan\n",
      "6699 nan\n",
      "6799 nan\n",
      "6899 nan\n",
      "6999 nan\n",
      "7099 nan\n",
      "7199 nan\n",
      "7299 nan\n",
      "7399 nan\n",
      "7499 nan\n",
      "7599 nan\n",
      "7699 nan\n",
      "7799 nan\n",
      "7899 nan\n",
      "7999 nan\n",
      "8099 nan\n",
      "8199 nan\n",
      "8299 nan\n",
      "8399 nan\n",
      "8499 nan\n",
      "8599 nan\n",
      "8699 nan\n",
      "8799 nan\n",
      "8899 nan\n",
      "8999 nan\n",
      "9099 nan\n",
      "9199 nan\n",
      "9299 nan\n",
      "9399 nan\n",
      "9499 nan\n",
      "9599 nan\n",
      "9699 nan\n",
      "9799 nan\n",
      "9899 nan\n",
      "9999 nan\n",
      "10099 nan\n",
      "10199 nan\n",
      "10299 nan\n",
      "10399 nan\n",
      "10499 nan\n",
      "10599 nan\n",
      "10699 nan\n",
      "10799 nan\n",
      "10899 nan\n",
      "10999 nan\n",
      "11099 nan\n",
      "11199 nan\n",
      "11299 nan\n",
      "11399 nan\n",
      "11499 nan\n",
      "11599 nan\n",
      "11699 nan\n",
      "11799 nan\n",
      "11899 nan\n",
      "11999 nan\n",
      "12099 nan\n",
      "12199 nan\n",
      "12299 nan\n",
      "12399 nan\n",
      "12499 nan\n",
      "12599 nan\n",
      "12699 nan\n",
      "12799 nan\n",
      "12899 nan\n",
      "12999 nan\n",
      "13099 nan\n",
      "13199 nan\n",
      "13299 nan\n",
      "13399 nan\n",
      "13499 nan\n",
      "13599 nan\n",
      "13699 nan\n",
      "13799 nan\n",
      "13899 nan\n",
      "13999 nan\n",
      "14099 nan\n",
      "14199 nan\n",
      "14299 nan\n",
      "14399 nan\n",
      "14499 nan\n",
      "14599 nan\n",
      "14699 nan\n",
      "14799 nan\n",
      "14899 nan\n",
      "14999 nan\n",
      "15099 nan\n",
      "15199 nan\n",
      "15299 nan\n",
      "15399 nan\n",
      "15499 nan\n",
      "15599 nan\n",
      "15699 nan\n",
      "15799 nan\n",
      "15899 nan\n",
      "15999 nan\n",
      "16099 nan\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "e = torch.tensor(0.00000001,device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(20000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3 + e * x ** 4\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        e -= learning_rate * e.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "        e.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12d738f96725d1c1f433a1d40c5369c2dd6b861cec3a8aa29acd662c91ac2528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
